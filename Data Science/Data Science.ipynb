{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science\n",
    "\n",
    "Course - [Data Science specialization](https://www.coursera.org/specializations/jhu-data-science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![separator2](https://i.imgur.com/4gX5WFr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Data Science Toolbox\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### What is Data Science and data scientist\n",
    "\n",
    "Data science is basically using data to answer questions. It can involve statistics, computer science, maths, data cleaning and formatting and data visualization.\n",
    "\n",
    "The need for data science has increased due to the generation of lot's of big data and need to extract some meaningful information from that data.\n",
    "\n",
    "**Data Scientist** can be defined as someone who combines programming, stats and domain knowledge to answer questions based on data.\n",
    "![Data Scientist](images/data_science-data_scientist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is data\n",
    "\n",
    "Data can be defined as \"A set of values of quantitative and qualitative variables\". __A set of values__ means that value population on which we are trying to discover something, __variables__ are the characterestics of the item, __qualitative__ means values which are non-numeric in nature and cannot be quantized, or we cannot tell the exact quantit and __quantitative__ means the values which we can tell the exaact amount of.\n",
    "\n",
    "In data science, first we ask the questions and then use data to find answer to those questions. It might happen that due to data limitations, we might have to refraim the question but the data science is driven by question and not data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of data analysis\n",
    "\n",
    "There are multiple types of data analysis that is used by data scientist to answer questions. There are broadly 6 categories of difficulties. In increasing order of difficulty, they are -\n",
    "1. **Descriptive** - \n",
    "![descriptive analysis](images/data_science-data_analysis-descriptive.png)\n",
    "This is usually the first analysis in the new dataset. In this, we just create a basic summary of the complete data that we have and do not try to find conclusions.There are additionaly analysis required if we want to create any interpretation.\n",
    "2. **Exploratory** - \n",
    "![exploratory analysis](images/data_science-data_analysis-exploratory.png)\n",
    "In this, we explore the relation between different variables in the data that weren't previously visible but do not confirm the causation of the relation, i.e., it does not tell how the change in 1 affects the another. This analysis should not be used as a final say on how and why the data is related to each other\n",
    "3. **Inferential** - \n",
    "![inferential analysis](images/data_science-data_analysis-inferential.png)\n",
    "Inferential analysis is commonly the goal of statistical modelling. Where you have a small amount of information to extrapolate and generalise that information to a larger group. If the data you collect is not from a representative sample of the population, the generalizations you infer won't be accurate for the population.\n",
    "4. **Predictive**\n",
    "![predictive analysis](images/data_science-data_analysis-predictive.png)\n",
    "5. **Causal** - \n",
    "![causal analysis](images/data_science-data_analysis-causal.png)\n",
    "Generally, causal analysis are fairly complicated to do with observed data alone. There will always be questions as to whether are these correlation driving your conclusions, or that the assumptions underlying your analysis are valid. It is seen frequently in scientific studies where scientists are trying to identify the cause of a phenomenon. But often getting appropriate data for doing a causal analysis is a challenge\n",
    "6. **Mechanistic** - \n",
    "![mechanistic analysis](images/data_science-data_analysis-mechanistic.png)\n",
    "Mechanistic analysis are not nearly as commonly used as the previous analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental design concepts\n",
    "\n",
    "As a data scientist, we need to design a proper experiement to answer the questions. Experimental design is organizing an experiment so that you have the correct data (and enough of it!) to clearly and effectively answer your data science question. Before going into analysis, it is imp to have a plan on how we will proceed with the analysis of data since incorrect analysis can lead to incorrect answers for our questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal of experimental design\n",
    "\n",
    "* Independent variable (AKA factor): The variable that the experimenter manipulates; it does not depend on other variables being measured. Often displayed on the x-axis.\n",
    "\n",
    "* Dependent variable: The variable that is expected to change as a result of changes in the independent variable. Often displayed on the y-axis, so that changes in X, the independent variable, effect changes in Y.\n",
    "\n",
    "* Hypothesis: an educated guess of the relationship between variables and the outcome of the experiment.\n",
    "\n",
    "* Confounder: An extraneous variable that may affect the relationship between the dependent and independent variables.\n",
    "\n",
    "* Sample size: The number of experimental subjects you will include in your experiment\n",
    "\n",
    "* A control group is a group of subjects for which the independent variable is not changed but still have their dependent variables measured. This allows to take care of the confounder. There are multiple ways to create control groups to take care of confounder\n",
    "    * Blinded - it means that the control group is not aware that the independent variables are note being changed for it. It is generally done by using __placebo__ effect.\n",
    "    * Randomization - it means randomly distributing the subjects to control and experiment group to decrease the concentration of the confounder in any one place\n",
    "    * Replication - repeating the experiment multiple times with different experiment groups to give weightage to the foundings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Big data\n",
    "\n",
    "Big data can be characterized by three qualities -\n",
    "![Big Data](images/data_science-big_data.png)\n",
    "* **Volume** - involves large data sets\n",
    "* **Velocity** - high speed of data generation\n",
    "* **Variety** - there are several different types of data available\n",
    "\n",
    "\n",
    "Given some of the qualities of big data above, you can already start seeing some of the challenges that may be associated with working with big data.\n",
    "\n",
    "* It is big: there is a lot of raw data that you need to be able to store and analyse;\n",
    "* It is constantly changing and updating: By the time you finish your analysis, there is even more new data you could incorporate into your analysis! Every second you are analysing, is another second of data you haven’t used!\n",
    "* The variety can be overwhelming: There are so many sources of information that it can sometimes be difficult to determine what source of data may be best suited to answer your data science question! And finally,\n",
    "* It is messy: You don’t have neat data tables to quickly analyse - you have messy data. Before you can start looking for answers, you need to turn your unstructured data into a format that you can analyse!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![separator2](https://i.imgur.com/4gX5WFr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Getting and cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Introduction\n",
    "\n",
    "Before performing any kind of data analysis on the data, it is important to have data ready for data analaysis since generally the data that we extract is not in a neat and tidy format that we can perform analysis on and we need to get our data ready for the same. In this section, we will go through the following - \n",
    "* Finding and extracting raw data.\n",
    "* Tidy data principles and how to make data tidy - taking the raw data and foramting into tidy data format to make it easy to use.\n",
    "* Practical implementation using Python ( for doing the same things in R, refer to this [course](https://www.coursera.org/learn/data-cleaning/home/welcome))\n",
    "\n",
    "For this section, having understanding of [exploratory data analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis) is useful.\n",
    "\n",
    "In real world we might have data in many forms, it could be neatly organized in an excel file, it could be a log text file data from which we need to extract some substrings, JSON data from API's and databases. Also the data can exist at multiple places or in multiple formats and we will have to collect all that data and format that to create a format which can be used for data analysis.\n",
    "\n",
    "The basic pipeline that we will have for data cleaning is - ` Raw Data -> Processing Script -> Tidy Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Raw vs Processed data\n",
    "\n",
    "Raw data \n",
    "* The original source of data \n",
    "* Often very hard to use in data analysis \n",
    "* Data analysis includes data processing/cleaning\n",
    "* Raw data may only need to be processed once but regardless on how many times the data is processed, we need to keep a record of all kinds of data processing performed \n",
    "\n",
    "Processed data - \n",
    "* Data is ready for analysis\n",
    "* Processing can include merging, subsetting, transforming, etc.\n",
    "* There may be standards of processing\n",
    "* **All steps should be recorded** - It is V.V.Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Tidy Data\n",
    "\n",
    "Tidy data is the data that we are actually performing the data analysis on. We transform the raw data that we read from various sources to tidy data using some data processing methods. After moving from raw data to tidy data, we should have the following\n",
    "1. The raw data \n",
    "2. The tidy data\n",
    "3. Codebook describing each variable in the tidy dataset and it's value, it can also be described as the metadata of the tidy data. It explains the structure of the tidy data\n",
    "4. An explicit and exact recipe to go from step 1 to step 2&3, we need an exact sequence of steps performed to get the tidy data and the codebook from the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Raw Data Definition\n",
    "\n",
    "The raw data is the rawest form of data that is available, i.e., unformatted and unprocessed data that we got directly from data sources like API, DB, excel sheet, raw text file, etc. We can say a raw data is raw if it has the following characterestics - \n",
    "* **No software has been executed on the data** - Apart from software or code executed to get the raw data, no other software or code or any other processing has been done on the data\n",
    "* No numbers have been manipulated\n",
    "* No data is removed from the dataset\n",
    "* data is not summarized in any way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidy Data Definition\n",
    "\n",
    "This is the target or end data that we want\n",
    "1. Each variable measured should be in a exactly one column\n",
    "2. Each different observation should be in a different row\n",
    "3. There should be 1 table for every kind of variable\n",
    "4. If Multiple tables are present, there should be a single column which helps each table link.\n",
    "\n",
    "It is better if the tidy data has the following -\n",
    "* A row should be at the top of the data set containing the variable names for each column\n",
    "* The variable names should be human readable\n",
    "* Data should be saved in 1 file per table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Code Book\n",
    "\n",
    "\n",
    "1. Information about the variables(including units) in the data set not contained in the tidy data, i.e., some kind of metadata about the variables which can help develop more understanding about the data\n",
    "2. Information about the summary choices made. For eg, whether mean or mediam was taken for a particluar column data \n",
    "3. Information about experimental study design - info about the way data was collected\n",
    "\n",
    "Some other important points - \n",
    "* Common format of document is word/text/markdown file\n",
    "* there should be a section called \"Study Design\" containing thorough description on how data was collected - It should say things like how it was decided what observation to collect, what is extracted from data base and what is not\n",
    "* A section called \"Code Book\" should be present containing all the variables and the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Instruction List\n",
    "\n",
    "We should be, at any time, be able to go back to the raw data and re process it to get the same tidy dataset. If it does not happen, then something is incorrect with the data processing pipeline and needs to be fixed.\n",
    "1. Ideally a computer script will perform this job\n",
    "2. Input for script is raw data\n",
    "3. Output for script is tidy data\n",
    "4. The script does not contain any parameters - Everything is fixed once initial data processing is done and there is no tweaking or modifying anything in the script. This will make sure that the behaviour of the script remains consistent.\n",
    "\n",
    "In some cases it is not possible to document each and every step in the script or sometimes we might not be able to create a script for every step and some manual operations need to be performed. In that case, the instructions should be provided like - \n",
    "1. Step 1 - Take the raw file, select the version of the software to be executed and set the parameter values (if any)\n",
    "2. Step 2 - Execute the software separately for each sample\n",
    "3. Step 3 - The column returns corresponds to any specific column in the final data\n",
    "\n",
    "While creating this, providing all the information is crucial, whatever information we can provide to the audience to let them know how the data was processed is beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Downloading Data\n",
    "\n",
    "Generally we will be downloading data from some location for us to be able to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking and creating directory\n",
    "\n",
    "Before downloading data, we need to create a path where we will keep all out data. For that, we need to check if our data folder is present or not, and if it is not present, create it. `os.path.isdir('data')` checks if a folder is present or not and returns `True` if a folder exists and `False` if it does not exist. `os.mkdir('data')` creates a folder if it does not exist.\n",
    "```python\n",
    "import os\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading a file from the web\n",
    "\n",
    "Now that we have created a data directory folder, next job is to get the data from the internet. We will use the [Baltimore Fixed Speed Cameras](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru) dataset for this example. Refer to the image to get the download URL \n",
    "\n",
    "<img src=\"images/data_science-baltimore_dataset.png\" width=\"80%\">\n",
    "\n",
    "```python\n",
    "import requests\n",
    "fileUrl = \"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n",
    "r = requests.get(fileUrl)\n",
    "with open('./data/cameras.csv', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "```\n",
    "\n",
    "We can list the files in the directory to check whether the download is completed or not\n",
    "```python\n",
    "import os\n",
    "os.listdir('data')\n",
    "```\n",
    "\n",
    "We can also check the date and time at the time of download to know that when the data file has been downloaded\n",
    "```python\n",
    "from datetime import datetime\n",
    "dateDownloaded = datetime.now()\n",
    "print(dateDownloaded)\n",
    "```\n",
    "\n",
    "It might take some time to download if the file is large. Also make sure to record when the file was downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "### Reading local flat files\n",
    "\n",
    "[Flat file](https://searchsqlserver.techtarget.com/definition/flat-file) are files which do not have any inherent structural interrelationship.\n",
    "\n",
    "Generally data is available in the following flat files formats - \n",
    "* CSV\n",
    "* Excel\n",
    "* XML\n",
    "* JSON\n",
    "\n",
    "Refer to [Reading Data](http://localhost:8888/lab/tree/Learning/Data%20analysis/Reading%20data.ipynb) in Python notebook for more info on how to read, write and manipulate data for these file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from data storages\n",
    "\n",
    "Apart from local flat files, we generally have [data storages](https://en.wikipedia.org/wiki/Data_storage) where all the data is present and we need to read from those. Some of them are as follows -\n",
    "* MySQL\n",
    "* HDF5\n",
    "* Web\n",
    "* API\n",
    "* Other sources like different DBs; zipped files(like gz or bz); other softwares like SAS, SPSS,  etc; images like jpeg, png; GIS data; musics data; etc.\n",
    "\n",
    "Refer to [Reading Data](http://localhost:8888/lab/tree/Learning/Data%20analysis/Reading%20data.ipynb) in Python notebook for more info on how to read, write and manipulate data for these data storages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Refer to [Data analysis notebook](http://0.0.0.0:8888/lab/tree/Learning/Data%20analysis/Data%20cleaning%20and%20visualization.ipynb)  to see how we can perform data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![separator2](https://i.imgur.com/4gX5WFr.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principles of Analytic Graphics\n",
    "\n",
    "Refer to [this book]() for more info.\n",
    "\n",
    "1. Show comparisions - Evidence for a hypothesis is always relative to another hypothesis, i.e., if we are going to compare any hyptothesis, let's say A, there is going to be an hypothesis B that we are going to compare it to. So whenever we describe our data or provide a summary, it is very important to ask what is the hypothesis compared to.\n",
    "2. Show causality/mechanism/explanation/systematic structure - this means that how we beleive the world works and accordingly or how a system is operating. What is the causal framework for thinking about the question? This basically means that what we think the reason could be behind why the hypothesis is working the way it is working.\n",
    "3. Show multivariate(more than 2 variable) data - Show as much data on a single plot as we can since the real world is multivariate. \n",
    "4. Integration of evidence - Completely integrate words, numbers, images, diagrams. Use as many modes of displaying evidence that we have. Do not be constraint by the tools to display evidence. For eg, if a tool only creates plots, do not just show plots in the summary. Do not let the tools drive the evidence. Should be able to combine different modes of evidence into a single presentation to make graphics or display as information rich as possible.\n",
    "5. Descibe and document the evidence with appropriate scales, labels, sources, etc - This is so that the idea that we are presenting has some credibility since anyone can refer to these details and come up with the information. \n",
    "6. Content is King - If the story is not interesting, no amount of presentation can make it interesting. Having quality content or data to show is very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Graphs \n",
    "\n",
    "Exploratory graphs are graphs that we make for ourselves so that we can look at the data and explore what's going on in the data. We use graphs to - \n",
    "* Understand data properties\n",
    "* Find patterns in data \n",
    "* Suggest modelling stratergies\n",
    "* Debug analysis\n",
    "* Communicate results\n",
    "\n",
    "Characterestics of exploratory graphs - \n",
    "* They are made very quickly - made on fly while looking at data\n",
    "* A large no of graphs are made to look at different aspects of data\n",
    "* goal is to develop personal understanding of dataset and what are the properties of data \n",
    "* Get a sense of data \n",
    "* axes/legends are generally cleaned up later\n",
    "* color/size are primarily used for information "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple summary of data\n",
    "\n",
    "We first check the simple summary of data. This can be done in following ways - \n",
    "* Five number summary - This is basically like `describe()` function in pandas which tells breif summary of the data like the min value, median, max, mean, 1st quartile and 3rd quartile\n",
    "* Boxplot\n",
    "* Histogram - This provides some more detail. We can set the bins to make the histogram more granular(more number of bars)\n",
    "* Density plot - A density plot is a smoothed, continuous version of a histogram estimated from the data\n",
    "\n",
    "We can also have overlaying features. It basically means that we overlay some feature onto a plot. This allows us to get some more information from the plot.\n",
    "\n",
    "We can do 2 dimension summaries using the following ways - \n",
    "* Multiple/overlayed 1-D plots\n",
    "* Scatterplot\n",
    "* Smooth Scatterplot\n",
    "\n",
    "We can do multidimension(>2) summaries in the following ways - \n",
    "* Multiple/overlayed 2-D plots\n",
    "* Using color/shape/size to add dimensions\n",
    "* Spinning plots\n",
    "* Actual 3-D plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
