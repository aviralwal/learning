1. Python
    * Syntax
    * How language works
    * Data types - string, integer, double
    * Data structures - sets, tuples, dictionary, lists
    * Functions, exception handling
    * OOPS
    * Gain expertise in Python
    
2. Library - 
    * Numpy - 
        * Understand the library like how to import, create array, braodcasting, reshaping array, get specific values
        * Learn indexing techniques properly
    * Pandas - 
        * Have to learn in depth
        * Useful in data handling
        * Understand Dataframes, data series and work on different operations in both
        * Focus more on feature engineering - handling missing data, scaling the data in specific range, etc

3. Statistics - 
    * Important to understand the basic concepts of statistics like Mean, median, mode, percentile, iqr, normal distribution, standard  normal distribution, Chebyshev's inequality, central limits theorum, correlation, co-variation, sub normal distribution, etc.
    * applied in data pre-processing, feature engineering and feature selection part, like for handling missing value, or if data is skewed, etc.
    * no need to learn the formula since most can be handled using libraries but understanding the practical implemntation and use cases, when and where they are used, why they are used is important
    
4.  Visualization - 
    * Library like seaborn and matplotlib.
    * seaborn is very good w.r.t visualizing statistical data. Has lot of in-built functions. Helps to understand how data is distributed. Makes feature engineering and feature selection easy.
    
5. Exploratory Data Analysis(EDA) - 
    *  Trying to find out as much information as we can from the dataset, like trying to find out differnt kind of variable(continuous, discrete, categorical) and best way to handle each of the variables, handling missing value, 
    * Understand how to scale down data(imp). It can be done using scaling technique like MinMax, Standard or LogNormal methods.
    * Always record the observation/output from the EDA process. This is helpful if we want to come back after some time to verify the observations.
    * Use a scale on library for EDA and data preprocessing techniques.
    
6. Understand ML Algo - 
    * Understand the maths and theoratical part first (imp) before moving to algorithm implementation.
    * Understand the algo parameters and hyperparameters.
    * Learn 15-20 imp ML algo and their maths
    * Understand the differences between different hyperparameter tuning algo

7. Deployment of ML model - 
    * It is necessary to learn. 
    * Learn different deployment techniques in different Cloud services(AWS, Azure, Heruko, etc) as well as using docker/kubernetes.
    * Flask and Django framework to create website/API
    
8. Deep Learning - 
    * 3 Imp algo - Artifical Neaural Network(ANN), Convolutional Neaural Network(CNN) & Recurrent Neaural Network(RNN)
    * Learn advanced techniques of CNN and Transfer Learning.
    * Understand these before moving to more advanced ones.
    * Use deployment techniques learned above to deploy. 
    * Imp library - tensorFlow, Keras, PyTorch
    
9. Databases -
    * SQL DB fo structured data
    * NoSQL DB like MongoDB for unstructured data
    
10.  Visualization tools - 
    * any of PowerBI, QlikSense or Tableau