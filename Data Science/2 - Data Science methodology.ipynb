{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Methodology\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition \n",
    "\n",
    "<center><img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DS0103EN/labs/images/lab1_fig2_datascience_methodology_flowchart.png\" width=\"60%\"></center>\n",
    "\n",
    "Data science methodology basically means the process that we follow and the methods that we apply in order to perform data science on the data. \n",
    "There are multiple ways we can define the methodology. A few of them are - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a series of questions\n",
    "Data Science methodology aims to answer the following 10 questions in the specifed sequence - \n",
    "\n",
    "* From problem to approach - \n",
    "    1. What is the problem you are trying to solve?\n",
    "    2. How can you use data to answer the question?\n",
    "\n",
    "\n",
    "* Working with data -\n",
    "    3. What data do you need to answer the question? \n",
    "    4. Where is the data coming from(identify all sources) and how will you get it?\n",
    "    5. Is the data that you collected representative of the problem to be solved?\n",
    "    6. What additional work is required to manipulate and work with the data?\n",
    "\n",
    "\n",
    "* Deriving the answer -\n",
    "    7. In what way can the data be visualized to get to the answer that is required?\n",
    "    8. Does the model used really answer the initial question or does it need to be adjusted?\n",
    "    9. Can you put the model into practice?\n",
    "    10. Can you get constructive feedback into answering the question?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Problem to approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business understanding (What is the problem you are trying to solve?)\n",
    "\n",
    "Data Science begins with seeking clarification and gaining business understanding. This is the first step in data science because having understanding about the problem to solve leads to determining which data can be used to solve the problem. This helps drive the Analytic approach. Also it is important to identify the right questions to ask instead of assuming the questions. Establishing a clearly defined question starts with understanding the goal of the person(for whom we are perfroming the data science activity) who is asking the question. Once the goal is understod, the next step is to identify the objectives that support the goal. This can help in prioritizing the tasks as well as plan how the process can be taken forward. Depending on the problem, different stakeholders will need to be engaged in the discussion to help determine requirements and clarify questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analytic approach (How can you use data to answer the question?)\n",
    "\n",
    "Once a strong understanding of the question is established(Step 1), the appropriate analytic approach can be selected. This means identifying what type of patterns will be needed to address the question most effectively. Some of the analytic approaches are as follows - \n",
    "* If the question is to determine probabilities of an action, then a predictive model might be used. \n",
    "* If the question is to show relationships, a descriptive model might be used. This would be one that would look at clusters of similar activities based on events and preferences. \n",
    "* For example if the question requires a yes/ no answer, then a classification model might be used(For eg, decision tree classification model). \n",
    "* Statistical analysis applies to problems that require counts.\n",
    "\n",
    "Will machine learning be utilized?\n",
    "Machine Learning is a field of study that gives computers the ability to learn without being explicitly programmed and can be used to identify relationships and trends in data that might otherwise not be accessible or identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Requirements to Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data requirements\n",
    "\n",
    "Once the business problem is identified and the analytic approach to be taken is fixed, the next step is to understanding what data is required, from where will it be sourced or collected, how to understand or work with it and how to prepare data to meet the required outcome.\n",
    "\n",
    "The data requirement step involves identifying the necessary data content(data required), formats and sources for initial data collection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data collection\n",
    "\n",
    "After the initial data collection is performed, an assessment by the data scientist takes place to determine whether or not they have what they need. \n",
    "\n",
    "In this phase the data requirements are revised and decisions are made as to whether or not the collection requires more or less data. Also the data scientist will have a good understanding of what they will be working with. Techniques such as descriptive statistics and visualization can be applied to the data set, to assess the content, quality, and initial insights about the data.\n",
    "\n",
    "Sometimes some of the data might not be available(for eg, the data source is not available or all the data is not loaded in the data source, etc). In this case, we can defer decisions about unavailable data and can look into it once it's available at a later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Understanding to Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Understanding\n",
    "\n",
    "Data understanding encompasses all activities related to constructing the data set. Essentially, the data understanding section of the data science methodology answers the question: Is the data that you collected representative of the problem to be solved?\n",
    "\n",
    "There are multiple techniques we can employ to gather that understanding. Some of them are\n",
    "* **Statistics** included Hearst, univariates, and statistics on each variable, such as mean, median, minimum, maximum, and standard deviation.\n",
    "* **pairwise correlations(univariates)** to see how closely certain variables were related, and which ones, if any, were very highly correlated, meaning that they would be essentially redundant, thus making only one relevant for modeling.\n",
    "* **Histograms** of the variables can be examined to understand their distributions. Histograms are a good way to understand how values or a variable are distributed, and which sorts of data preparation may be needed to make the variable more useful in a model. For example, for a categorical variable that has too many distinct values to be informative in a model, the histogram would help them decide how to consolidate those values.\n",
    "\n",
    "The **univariates**, **statistics**, and **histograms** are also used to assess data quality.\n",
    "\n",
    "From the information provided, certain values can be re-coded or perhaps even dropped if necessary, such as when a certain variable has many missing values. The question then becomes, does \"missing\" mean anything? Sometimes a missing value might mean \"no\", or \"0\" (zero), or at other times it simply means \"we don't know\". Or, if a variable contains invalid or misleading values, such as a numeric variable called \"age\" that contains 0 to 100 and also 999, where that \"triple-9\" actually means \"missing\", but would be treated as a valid value unless we corrected it.\n",
    "\n",
    "The more one works with the problem and the data, the more one learns and therefore the more refinement that can be done within the model, ultimately leading to a better solution to the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preperation\n",
    "\n",
    "* **Transforming data** is the process of getting the data into a state where it may be easier to work with. Specifically, the data preparation stage of the methodology answers the question: **\"What are the ways in which data is prepared?\"**\n",
    "* To work effectively with the data, it must be prepared in a way that **addresses missing or invalid values and removes duplicates**, toward ensuring that everything is properly formatted.\n",
    "* **Feature engineering** is also part of data preparation. It is the process of using domain knowledge of the data to create features that make the machine learning algorithms work. A feature is a characteristic that might help when solving a problem. Features within the data are important to predictive models and will influence the results you want to achieve. Feature engineering is critical when machine learning tools are being applied to analyze the data.\n",
    "\n",
    "Together with data collection and data understanding, data preparation is the most **time-consuming phase** of a data science project, typically taking **seventy percent and even up to even ninety percent** of the overall project time. Automating some of the data collection and preparation processes in the database, can **reduce this time to as little as 50 percent**. This time savings translates into increased time for data scientists to focus on creating models. The data preparation phase sets the stage for the next steps in addressing the question. While this phase may take a while to do, **if done right the results will support the project**. If this is skipped over, then the outcome will not be up to par and may have you back at the drawing board. It is vital to take your time in this area, and use the tools available to automate common steps to accelerate data preparation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Modelling to Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelling\n",
    "\n",
    "Modelling is the stage in the data science methodology where the data scientist has the chance to sample the sauce and determine if it's bang on or in need of more seasoning. The end goal is to move the data scientist to a point where a data model can be built to answer the question. In this stage, the data scientist will play around with different algorithms to ensure that the variables being used are actually required. The data supports the answering of the question and sets the stage for the outcome. \n",
    "\n",
    "In this stage of the methodology, model evaluation, deployment, and feedback loops ensure that the answer is near and relevant. Constant refinement, adjustments and tweaking are necessary within each step to ensure the outcome is one that is solid.\n",
    "\n",
    "Data Modelling focuses on developing models that are 1 of -\n",
    "* Descriptive - An example of a descriptive model might examine things like: if a person did this, then they're likely to prefer that.\n",
    "* Pedictive - A predictive model tries to yield yes/no, or stop/go type outcomes.\n",
    "\n",
    "These models are based on the analytic approach that was taken, either statistically driven or machine learning driven.\n",
    "\n",
    "The data scientist will use a training set for predictive modelling. A training set is a set of historical data in which the outcomes are already known. The training set acts like a gauge to determine if the model needs to be calibrated.\n",
    "\n",
    "The success of data compilation, preparation and modelling, depends on the understanding of the problem at hand, and the appropriate analytical approach being taken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "A model evaluation goes hand-in-hand with model building as such, the modeling and evaluation stages are done iteratively. Model evaluation is performed during model development and before the model is deployed. Evaluation allows the quality of the model to be assessed but it's also an opportunity to see if it meets the initial request. Evaluation answers the question: **\"Does the model used really answer the initial question or does it need to be adjusted?\"**\n",
    "\n",
    "Model evaluation can have two main phases.\n",
    "* The first is the diagnostic measures phase, which is used to ensure the model is working as intended. If the model is a predictive model, a decision tree can be used to evaluate if the answer the model can output, is aligned to the initial design. It can be used to see where there are areas that require adjustments. If the model is a descriptive model, one in which relationships are being assessed, then a testing set with known outcomes can be applied, and the model can be refined as needed.\n",
    "* The second phase of evaluation that may be used is statistical significance testing. This type of evaluation can be applied to the model to ensure that the data is being properly handled and interpreted within the model. This is designed to avoid unnecessary second guessing when the answer is revealed.\n",
    "\n",
    "\n",
    "The ROC(receiver operating characteristic curve) curve is a useful diagnostic tool in determining the optimal classification model. This curve quantifies how well a binary classification model performs, declassifying the yes and no outcomes when some discrimination criterion is varied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Deployment to Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deployment\n",
    "\n",
    "While a data science model will provide an answer, the key to making the answer relevant and useful to address the initial question, involves getting the stakeholders familiar with the tool produced. In a business scenario, stakeholders have different specialties that will help make this happen, such as the solution owner, marketing, application developers, and IT administration. \n",
    "\n",
    "Once the model is evaluated and the data scientist is confident it will work, it is deployed and put to the ultimate test. Depending on the purpose of the model, it may be rolled out to a limited group of users or in a test environment, to build up confidence in applying the outcome for use across the board."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feedback\n",
    "\n",
    "Once in play, feedback from the users will help to refine the model and assess it for performance and impact. The value of the model will be dependent on successfully incorporating feedback and making adjustments for as long as the solution is required. \n",
    "The feedback process is rooted in the notion that, the more you know, the more that you'll want to know. Once the model is evaluated and the data scientist is confident it'll work, it is deployed and put to the ultimate test: **actual, real-time use in the field.**\n",
    "\n",
    "* Once the model is deployed, data is gathered on performance of model based on the parameters relevant to the model and the accuracy of model is measured. \n",
    "* Then the model is refined, based on all of the data compiled after model implementation and the knowledge gained throughout these stages. After feedback and practical experience with the model, it might also be determined that adding new data(that might have been previously deferred) could be worth the investment of effort and time. We also have to allow for the possibility that other refinements might present themselves during the feedback stage. Also, the intervention actions and processes would be reviewed and very likely refined as well, based on the experience and knowledge gained through initial deployment and feedback.\n",
    "* Finally, the refined model and intervention actions would be redeployed, with the feedback process continued throughout the life of the Intervention program.\n",
    "\n",
    "Throughout the Data Science Methodology, each step sets the stage for the next. Making the methodology cyclical, ensures refinement at each stage in the game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlining the major processes\n",
    "Another way we can define the data science methodology is as follows -\n",
    "\n",
    "1. **Understanding the (business) problem** - The first step is to understand the actual problem that we want to solve using the data. Ask right questions and gather as much clarity as possible on what exactly is required to be found out from the data. More clear and concise is the question, easier will be to find the answer. It’s important that at the end of this stage, you have all of the information and context you need to solve this problem.\n",
    "2. **Data acquisition** - Next we acquire data to solve the problem. This data can be acquired from multiple sourecs like internet, databases, logs, sensors, etc\n",
    "3. **Data preparation** - Once we have the data, we need cleanup the data, condense it and format it in such a manner that it can be used for further processing. This is one of the most important steps in data science process since this makes the foundation for further processes.\n",
    "4. **Exploratory data analysis** - Afer data preperation, we perform some exploratory analysis which gives us a rough idea about the data, what it represents. This step is just to get hang of what data is trying to communicate. We are not performing any proper analysis or finding any inferences. \n",
    "5. **Data modeling** - After Exploratry Data Analysis is completed, we use the data to create statistical and mathematical predictive models(this is generally where ML comes in) to be able to perform predictions in future on new data based on learning from the current models.\n",
    "6. **Visualization and communication** - Finally after all is done, we need to visualize our finding and be able to communicate to the one's we want to, like clients, boss, manager, decision makers, etc, since it will all be of less use if we are unable to properly communicate whatever we have found out.\n",
    "7. **Deploy & maintenance** - After we have created our data models, presented it and everything seems to work, the last step is to deploy the data model so that we can use it in real world to process data and get the desired output. Once the model is deployed, we need to mantain the model and  update it according to feedback. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing into broad steps\n",
    "\n",
    "In the process of DS, there are multiple processes that are involved. These can be broadly classified as - \n",
    "1. Planning - \n",
    "    1. Define goals for the project\n",
    "    2. Organize various resorces(people) for the project\n",
    "    3. Coordinate people, i.e., defining who will do what, how they will do\n",
    "    4. Schedule project\n",
    "2. Data preperation - The steps in these process can bounce front and back, i.e., we might jump among steps based on requirements.\n",
    "    1. Get data - can come from multiple sources and can be in multiple formats\n",
    "    2. Clean data - the data needs to be cleaned so that it can be used. This is a large part of DS process\n",
    "    3. Explore data - what the data looks like, shape, how it's distributed and some more insights about data\n",
    "    4. Refine data - choosing varibles and cases to include/exclude, making data transformation\n",
    "3. Data modelling -\n",
    "    1. Create model - We create the data model, like regression model, neural network model, etc.\n",
    "    2. Validate model - Checking if the model is valid using different techniques.\n",
    "    3. Evaluate model - What does the model actually mean and what does it tell about the data.\n",
    "    4. Refine model - Make the model better by tweaking some settings\n",
    "4. Follow up \n",
    "    1. Present model - Once the model is created, the model needs to be present(generally the work is being done for a client or third patry), i.e., share the insights gained from the model in a meaningful manner.\n",
    "    2. Depliy model - The model needs to be deployed so that it can be used in real world\n",
    "    3. Revisit model - Generally the data used for modelling is not all the data or not proper representaion of the real world. So we would revisit the model based on new data or changes in data and update it accordingly.\n",
    "    4. Archive assets - document all the finding so that anyone repeat the analysis and cpontinue the development.\n",
    "    \n",
    "We see that DS is not only a technical(coding) field. There are planning, presentation and implementation parts too. Also contextual skills,i.e., knowing how it works and how it will be implemented, also matter. Also, since there are multiple steps involved in this process, we should go 1 step at a time so that there is less backtracking and in the end, increase productivity. Tech is means to get insights in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods used in DS \n",
    "\n",
    "<img src=\"images/data_science-domains.png\">\n",
    "\n",
    "For each process that we perform in Data Science, there are several methods that we can use based on our requirements. Some of the methods used in each process are defined below - \n",
    "\n",
    "* Sourcing - How to get data that is used in DS. It is important to remember GIGO(Garbage In, Garbage Out) which means if the data that is used is bad then we will not get any meaningful insights from the data. It is important to pay attention to metrics used for gathering data and what do they exactly meanand check the quality of the data. Some of the methods used are -\n",
    "    * Using existing data - Data that is already in hand, or we can say readily available to consume. It can be \n",
    "        * in house - data present in the company\n",
    "        * open data - data that is made public by govt or other organizations\n",
    "        * third-party - data that is bought from a vendor which is ready to be plugged in and ready to use\n",
    "    * Using Data API - Allows applications to communicate with each other. Common way to get web data and allows easy import of the same directly\n",
    "    * Scraping Data - This is for data in web which do not have API. Used for data like HTML, PDF, images, etc. It can be done by using specical programs or by writing programs in Python/R.\n",
    "    * Creating own data - Allows to get what is exactly required. It can be done using interviews, surveys, experiments, etc. These methods require some kind of special training to get data.\n",
    "* Coding - code required to obtain, manipulate, analyse data. This can be any technology that allows us to manipulate data according to our needs. It is important that the tools are not DS, they are enablers to DS and help us to reach the end goal, i.e., insights about data. Also we do not need to learn each and every thing. Sometimes a few tools allow us to do a whole lot.\n",
    "    * Apps - Specialized application for working with data. \n",
    "        * Spreadsheets - google sheets, excel, etc\n",
    "        * Tableau, PowerBI, QlikSense, etc\n",
    "        * SAS\n",
    "        * etc\n",
    "    * Data Formats - Special formats for web data\n",
    "        * HTML\n",
    "        * XML\n",
    "        * JSON\n",
    "        * etc\n",
    "    * Code - Programming languages that provide full control on what we want to interact with data.\n",
    "        * R\n",
    "        * Ptyhon\n",
    "        * SQL\n",
    "        * NoSQL\n",
    "        * Bash(command line)\n",
    "        * Regex\n",
    "        * etc\n",
    "* Math - Math behind DS methods. It makes the foundation of what we do in DS. Having understaning of math is important because we need to understand which procedures to use & why, know what to do when things don't work right like when we get impossible result in specific result we can use math to understand why it is happening and why is the method not working in the situaten, and some math is easier to do in hand than in a system. Knowing math is not necessary but it helps a lot if we have understaning of it. Some kind of math required are -\n",
    "    * Algebra -\n",
    "        * Elementary algebra\n",
    "        * Linear(matrix) algebra - foundation of many calculations\n",
    "        * System of linear equations\n",
    "    * Calulus\n",
    "    * Big O\n",
    "    * Probability\n",
    "    * Bayes' theorum\n",
    "* Stats - Methods to summarize and analyze data. \n",
    "    * Explore data -  \n",
    "        * exploratory graphics\n",
    "        * Exploratry stats\n",
    "        * Descriptive stats\n",
    "    * Inference - infer something about data based on some patterns or we can say infer something about population using samples. Some methods can be\n",
    "        * Hypothesis testing\n",
    "        * Estimation\n",
    "        * etc\n",
    "    * Details - \n",
    "        * Feature selection - selecting variables that should be used in data analysis\n",
    "        * Problems \n",
    "        * Validation - validate the statistical model created to check whether it actually works on the complete data.\n",
    "        * Estimators\n",
    "        * Fit\n",
    "* ML - methods to finding cluster, predicting categories and scores. Goal of ML is to work in data space. It might be used for dimension reduction, clustering, detecting anomlies in data space. Some of the methods are \n",
    "    * Categorization of data - Logistic regression, kNN, Naive Bayes, Decision trees, SVM, Neural nets\n",
    "    * Clustering - K-Means\n",
    "    * Predictions - Linear regression, Poisson regression, Ensemble models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communicating in Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretatibility\n",
    "\n",
    "The goal of Data Science is to allow us to tell people a data-driven story and lead them in it. In analysis, we are trying to Solve for Value. The value is basically the thing that we want to solve and the analysis we do on data is not value, it is the combination of analysis and the Story that creates the value, or we can say that `Analysis x Story = Value`. So we want to maximize our story also so that we can maximize the value that we derive from all of the process. Maximize the Value is our end goal in all the process.\n",
    "\n",
    "Goals - \n",
    "* Analysis is goal driven\n",
    "* Story should match the goals\n",
    "* Answer questions clearly and unambigiously\n",
    "\n",
    "One important thing to understand is that the client( anyone for which we are doing the process) is not the same as us, they do not see what we see due to which there are certian psychological abstractions that we have to be aware about - \n",
    "* Egocentrism - we cannot assume that the other people see, understand and know what we know since if that had been the case, we would not have been required for the DS process. So we have to put info in terms that the client can understand. For that we have to get out of our center or comfort zone.\n",
    "* False consensus - The idea that everyone knows a particular thing. This again is not true since if it was the case then we would not have been needed. Important to understand that the people we are communicating with are coming from different background, have different knowledge, interpretation and expertise and we need to compensate for that.\n",
    "* Anchoring - When we give a particular first impression, that is used as an anchor or reference point for things that are to proceed. So it is important that if we are going to change the points that they have in their mind, it is important to set the correct anchor point.\n",
    "* Clartiy at each step - It is imp to have clarity at each step and provide proper explanation.\n",
    "\n",
    "When we are explaining the project to the client, we can proceed in following fashion - \n",
    "* State the question that we are answering\n",
    "* Give the answer\n",
    "* Qualify as needed with information\n",
    "* Go in order of top to bottom and repeat the process for each question. Answer the question in order that makes sense.\n",
    "* Discuss process sparingly - Most of the times the client is not interested in understanding the process followed to solve the problem. It is genereally that the answer is required and that the process that is used to reach to the answer is a good method. So do this only if needed.\n",
    "\n",
    "One important thing to remember that analysis is the exercise simplicification. We are essentially taking the overwhelmingness of the data and boiling down to essential parts that serve the needs of the client. Everything should be made as simple as possible, but no simpler, or be minimally sufficient. Few tips to keep things simple - \n",
    "* Use more charts and less text in presentations\n",
    "* Simplify charts\n",
    "* Avoid tables since they are difficult to read\n",
    "* Less text - having more and more text will reduce the simplicity. Try to use as much charts and tables to communicate as possible.\n",
    "\n",
    "Important pointers - \n",
    "* Stories give value to data analysis.\n",
    "* Important to address client's goals.\n",
    "* Be minimally sufficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actionable insights\n",
    "\n",
    "Information that can be used productively for something. Data is for doing. The analysis is for doing something from the finding.\n",
    "\n",
    "We should be able to give the next steps to the client, let them know what they need to do right now and justify that action with data. Be specific in action and the action should be doable by client. Also each step should build on each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation vs causation\n",
    "\n",
    "Our data gives our correlation(what is associated with what). But what we are searching for is causation(what causes something). So we need to get from correlation that we got from data to causation that can be used to take the next step. Some of the ways to reach for correlation to causation are - \n",
    "* Experimental studies - randomized control trials. Theoratically simplest but tricky in real world.\n",
    "* Quasi-experiments - collectionb of methods that ise non-randomized(generally observational) data to get causal inference.\n",
    "* Theory & experience - Research based theory and domain-specific experience. Here we can rely on client's information. They can help interpret the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Social factors affecting data\n",
    "\n",
    "Having social understanding is also important. Some of the important social understanding are -  \n",
    "* Aware about client's mission - The recommendations that we provide to the client should be inline to the client's mission\n",
    "* The recommendation should also match with client's identity, i.e., what they exactly are\n",
    "* Business context - which environments does the client deal with, regulations, competition, etc\n",
    "* Social context - relations outside and within organization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation graphics\n",
    "\n",
    "When we are providing insights and data to the client, one important aspect is to provide clear and crisp graphics that provide proper representation of what we want to communicate.\n",
    "\n",
    "There can be multiple goals of graphics based on the kind of graphics we are working with. Some of them are -  \n",
    "* Exploratory graphics - used by us(as analysts). This requires speed and responsiveness. This is done to be used by us to understand data. They are quick, effective, simple grpahics.\n",
    "* Presentation graphics - They are used by the clients and hence require clarity and narrative flow. \n",
    "\n",
    "Graphics used for presenting are not same as graphics used for exploring.\n",
    "In both, we should be clear and focused on what we are trying to tell.\n",
    "It is important to create a strong narrative that gives a different level of perspective and answers the clients questions as we move forward and is able to anticipate client questions before-hand and is able to give reliable, strong information and provide confidence to the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clarity vs distractions\n",
    "\n",
    "While creating graphics, there are multiple things that can go wrong. Some of them are - \n",
    "* Colors - they can become problem if not used problems\n",
    "* 3D - multidimensional graphics are generally more distracting \n",
    "* Interaction - Interactive graphics more often become a distraction because the client get's distracted by the interactions and moves them away from the message that we are trying to communicate them.   \n",
    "* Animation - flat, static graphics are generally more informative since they have fewer distractions in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducible research\n",
    "\n",
    "Data science projects are not one's which are done once and then over. They are generally incremental, cumilative and adaptive. \n",
    "\n",
    "It is important to be able to show the work done(the process of how we performed the analysis and reached the current state of things) because -\n",
    "* Revising - We may have to revise the work at a later time\n",
    "* Borrowing - We may use the working of the current project in other projects\n",
    "* Hand off - We generally have to hand off the work to some one else\n",
    "* Accountability - Able to show that work was done in a proper manner and that the conclusions are justifiable\n",
    "\n",
    "Archive the data - making the information available\n",
    "* All data sets - raw,unprocessed data as well as data obtained in every step in data processing and the final processed data.\n",
    "* All the code used to process and analyse data.\n",
    "* Comment liberally - The comments should be used as much as possible in order to explain each and every thing.\n",
    "\n",
    "Process -\n",
    "* We should be able to explain why we did something a particular way\n",
    "* Include choices, consequence of choices and backtracking\n",
    "\n",
    "Future proofing the work - \n",
    "* Data - store data in non-proprietary formats, like CSV so that we can access it easily at any point of time in the future without worrying about proprietary softwares, licensing, etc.\n",
    "* Storage - place the file in a secure accessible location(like github)\n",
    "* Code - Use dependency management(like virtual environment in Python) so that we make sure that the package versions used are the ones that we did the analysis with, since sometimes the packages get updated and then things do not work in the manner we expect them to.\n",
    "\n",
    "We should be able to explain ourself. For that we can use a notebook, it can be either physical or something like Jupyter notebook \n",
    "\n",
    "We should be able to support collaboration on the work. This will allow to future proof work. Also explain the narrative so that people can use those to vaildate that the work is justifiable and reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
