{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Sourcing or Getting and cleaning data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "To perform any kind of data science or data analysis, we need data. No data means no data science. Before performing any kind of data analysis on the data, it is important to have data ready for data analaysis since generally the data that we extract is not in a neat and tidy format that we can perform analysis on and we need to get our data ready for the same. In this section, we will go through the following - \n",
    "* Finding and extracting raw data.\n",
    "* Tidy data principles and how to make data tidy - taking the raw data and formating into tidy data format to make it easy to use.\n",
    "* Practical implementation using Python\n",
    "\n",
    "\n",
    "In real world we might have data in many forms, it could be neatly organized in an excel file, it could be a log text file data from which we need to extract some substrings, JSON data from API's and databases. Also the data can exist at multiple places or in multiple formats and we will have to collect all that data and format that to create a format which can be used for data analysis.\n",
    "\n",
    "The basic pipeline that we will have for data cleaning is - ` Raw Data -> Processing Script -> Tidy Data`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw vs Processed data\n",
    "\n",
    "Raw data \n",
    "* The original source of data \n",
    "* Often very hard to use in data analysis \n",
    "* Data analysis includes data processing/cleaning\n",
    "* Raw data may only need to be processed once but regardless on how many times the data is processed, we need to keep a record of all kinds of data processing performed \n",
    "\n",
    "Processed data - \n",
    "* Data is ready for analysis\n",
    "* Processing can include merging, subsetting, transforming, etc.\n",
    "* There may be standards of processing\n",
    "* **All steps should be recorded** - It is V.V.Important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidy Data\n",
    "\n",
    "Tidy data is the data that we are actually performing the data analysis on. We transform the raw data that we read from various sources to tidy data using some data processing methods. After moving from raw data to tidy data, we should have the following\n",
    "1. The raw data \n",
    "2. The tidy data\n",
    "3. Codebook describing each variable in the tidy dataset and it's value, it can also be described as the metadata of the tidy data. It explains the structure of the tidy data\n",
    "4. An explicit and exact recipe to go from step 1 to step 2&3, we need an exact sequence of steps performed to get the tidy data and the codebook from the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Raw Data Definition\n",
    "\n",
    "The raw data is the rawest form of data that is available, i.e., unformatted and unprocessed data that we got directly from data sources like API, DB, excel sheet, raw text file, etc. We can say a raw data is raw if it has the following characterestics - \n",
    "* **No software has been executed on the data** - Apart from software or code executed to get the raw data, no other software or code or any other processing has been done on the data\n",
    "* No numbers have been manipulated\n",
    "* No data is removed from the dataset\n",
    "* data is not summarized in any way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidy Data Definition\n",
    "\n",
    "This is the target or end data that we want\n",
    "1. Each variable measured should be in a exactly one column\n",
    "2. Each different observation should be in a different row\n",
    "3. There should be 1 table for every kind of variable\n",
    "4. If Multiple tables are present, there should be a single column which helps each table link.\n",
    "\n",
    "It is better if the tidy data has the following -\n",
    "* A row should be at the top of the data set containing the variable names for each column\n",
    "* The variable names should be human readable\n",
    "* Data should be saved in 1 file per table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Code Book\n",
    "\n",
    "\n",
    "1. Information about the variables(including units) in the data set not contained in the tidy data, i.e., some kind of metadata about the variables which can help develop more understanding about the data\n",
    "2. Information about the summary choices made. For eg, whether mean or mediam was taken for a particluar column data \n",
    "3. Information about experimental study design - info about the way data was collected\n",
    "\n",
    "Some other important points - \n",
    "* Common format of document is word/text/markdown file\n",
    "* there should be a section called \"Study Design\" containing thorough description on how data was collected - It should say things like how it was decided what observation to collect, what is extracted from data base and what is not\n",
    "* A section called \"Code Book\" should be present containing all the variables and the data types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Instruction List\n",
    "\n",
    "We should be, at any time, be able to go back to the raw data and re process it to get the same tidy dataset. If it does not happen, then something is incorrect with the data processing pipeline and needs to be fixed.\n",
    "1. Ideally a computer script will perform this job\n",
    "2. Input for script is raw data\n",
    "3. Output for script is tidy data\n",
    "4. The script does not contain any parameters - Everything is fixed once initial data processing is done and there is no tweaking or modifying anything in the script. This will make sure that the behaviour of the script remains consistent.\n",
    "\n",
    "In some cases it is not possible to document each and every step in the script or sometimes we might not be able to create a script for every step and some manual operations need to be performed. In that case, the instructions should be provided like - \n",
    "1. Step 1 - Take the raw file, select the version of the software to be executed and set the parameter values (if any)\n",
    "2. Step 2 - Execute the software separately for each sample\n",
    "3. Step 3 - The column returns corresponds to any specific column in the final data\n",
    "\n",
    "While creating this, providing all the information is crucial, whatever information we can provide to the audience to let them know how the data was processed is beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n",
    "\n",
    "Generally we will be downloading data from some location for us to be able to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking and creating directory\n",
    "\n",
    "Before downloading data, we need to create a path where we will keep all out data. For that, we need to check if our data folder is present or not, and if it is not present, create it. `os.path.isdir('data')` checks if a folder is present or not and returns `True` if a folder exists and `False` if it does not exist. `os.mkdir('data')` creates a folder if it does not exist.\n",
    "```python\n",
    "import os\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading a file from the web\n",
    "\n",
    "Now that we have created a data directory folder, next job is to get the data from the internet. We will use the [Baltimore Fixed Speed Cameras](https://data.baltimorecity.gov/Transportation/Baltimore-Fixed-Speed-Cameras/dz54-2aru) dataset for this example. Refer to the image to get the download URL \n",
    "\n",
    "<img src=\"images/data_science-baltimore_dataset.png\" width=\"80%\">\n",
    "\n",
    "```python\n",
    "import requests\n",
    "fileUrl = \"https://data.baltimorecity.gov/api/views/dz54-2aru/rows.csv?accessType=DOWNLOAD&bom=true&format=true\"\n",
    "r = requests.get(fileUrl)\n",
    "with open('./data/cameras.csv', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "```\n",
    "\n",
    "We can list the files in the directory to check whether the download is completed or not\n",
    "```python\n",
    "import os\n",
    "os.listdir('data')\n",
    "```\n",
    "\n",
    "We can also check the date and time at the time of download to know that when the data file has been downloaded\n",
    "```python\n",
    "from datetime import datetime\n",
    "dateDownloaded = datetime.now()\n",
    "print(dateDownloaded)\n",
    "```\n",
    "\n",
    "It might take some time to download if the file is large. Also make sure to record when the file was downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading local flat files\n",
    "\n",
    "[Flat file](https://searchsqlserver.techtarget.com/definition/flat-file) are files which do not have any inherent structural interrelationship.\n",
    "\n",
    "Generally data is available in the following flat files formats - \n",
    "* CSV\n",
    "* Excel\n",
    "* XML\n",
    "* JSON\n",
    "\n",
    "Refer to [Reading Data](http://localhost:8888/lab/tree/Learning/Data%20analysis/Reading%20data.ipynb) in Python notebook for more info on how to read, write and manipulate data for these file formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading from data storages\n",
    "\n",
    "Apart from local flat files, we generally have [data storages](https://en.wikipedia.org/wiki/Data_storage) where all the data is present and we need to read from those. Some of them are as follows -\n",
    "* MySQL\n",
    "* HDF5\n",
    "* Web\n",
    "* API\n",
    "* Other sources like different DBs; zipped files(like gz or bz); other softwares like SAS, SPSS,  etc; images like jpeg, png; GIS data; musics data; etc.\n",
    "\n",
    "Refer to [Reading Data](http://localhost:8888/lab/tree/Learning/Data%20analysis/Reading%20data.ipynb) in Python notebook for more info on how to read, write and manipulate data for these data storages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Refer to [Data analysis notebook](http://0.0.0.0:8888/lab/tree/Learning/Data%20analysis/Data%20cleaning%20and%20visualization.ipynb)  to see how we can perform data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
