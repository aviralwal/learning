{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Toolkit\n",
    "\n",
    "Data Science is a process of answering questions through data and in this process there are multiple tools that we can use in other to fulfill it. There is no 1 tool fits all and each tool has it's own pros and cons.\n",
    "\n",
    "There are multiple tasks that we perform in data science. We can define them broadly as - \n",
    "* Data management - process of persisting and retreiving data\n",
    "* Data integration and transformation(or ETL - Extract, Transform and Load or data refining and cleaning) - process of retreiving data from remote data source, transforming data and loading into local data system\n",
    "* Data Visualization - part of intial data exploration and final deliverable \n",
    "* Model Building - creating ml or dl model\n",
    "* Model Deployment - making model available to others application\n",
    "* Model monitoring and assessment - continuous quality checks on deployed models for accuracy, fairness, etc \n",
    "* Code asset management - code subversioning(like git) and team collaberation\n",
    "* Data asset management - data backup, replication and access mgmt\n",
    "* Development Environment(or IDE) setup - environment to help Data Scientist develop, test and deploy works\n",
    "* Execution env setup- tools where data processing, modeling training and deployment take place"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools for Data Science tasks - \n",
    "\n",
    "To fulfill the data science tasks, there are multiple tools, both open-source and commercial. Each type of tool has it's own pros and cons and there is no 1 tool fits all. The selection criteria for a specific tool depends on our capabilities and requirement. These tools are standalone tools and do not include programming libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Source tools \n",
    "\n",
    "* Data management -\n",
    "    * realtional db - MySQL, PostgreSQL\n",
    "    * NoSQL db - MongoDB, CouchDB\n",
    "    * File system based tools - HDFS(Hadoop file system), CEPH(cloud based file system), elasticsearch\n",
    "    \n",
    "    \n",
    "* Data integration - \n",
    "    * Apache airflow\n",
    "    * KubeFlow - pipeline execution on kubernetes\n",
    "    * Apache Kafka - \n",
    "    * Apache nifi\n",
    "    * Apache SparkSQL\n",
    "    * Node RED - provides visual editor\n",
    "    \n",
    "    \n",
    "* Data Visualization - These comprise of programming libraries that can be used to create visualizations and tools that inherently provide visualization capabilities. Some of the common tools are \n",
    "     * Hue - visualization from SQL query\n",
    "     * Kibana - For Elasticsearch\n",
    "     * Apache Superset\n",
    "     \n",
    "     \n",
    "* Model Deployment - Once the model is developed, the model should be consumable by others(via APIs)\n",
    "    * Apache PredicationIO - Supports Apache Spark ML models for deployment\n",
    "    * Seldon - Supports majority of frameworks. Runs on top of kubernetes or openshift\n",
    "    * mleap\n",
    "    * tensorflow service - used to deploy tensorflow models\n",
    "    \n",
    "    \n",
    "* Model monitoring - Once a model is deployed, imp to track performance as new data arrives in order to check if the model is outdated or not.\n",
    "    * ModelDB - Machine model DB. Supports Apache Spark pipeline and scikit\n",
    "    * Prometheus\n",
    "    \n",
    "    \n",
    "* Code asset management - \n",
    "    * git - platforms like github, gitlab, bitbucket, etc\n",
    "    \n",
    "    \n",
    "* Data asset management - \n",
    "    * Apache atlas\n",
    "    * ODPi Egeria\n",
    "    * kylo - data lake management platform\n",
    "    \n",
    "    \n",
    "* Development Environment - \n",
    "    * Jupyter\n",
    "    * Apache Zepplin\n",
    "    * R Studio\n",
    "    * Spyder\n",
    "    \n",
    "    \n",
    "* Execution Environments - \n",
    "    * Apache Spark - cluster computing framework, widely used. Key property is linear scalibility(double servers means double capacity). It is a batch data processing engine\n",
    "    * Apache Flink - stream processing engine(can process real time data stream)\n",
    "    * Ray(riselab)\n",
    "    \n",
    "    \n",
    "* Fully integrated visual tools - These are tools which help in many or all of the data science tasks define above and also do not require must of programming knowledge to use. Some of them are - \n",
    "    * Knime\n",
    "    * orange - easier to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commercial tools\n",
    "\n",
    "* Data management -\n",
    "    * Oracle DB\n",
    "    * Miscrosoft SQL Server\n",
    "    * AWS DynamoDB\n",
    "    * Cloudant CouchDB\n",
    "    * DB2\n",
    "    * etc\n",
    "    \n",
    "    \n",
    "* Data integration - \n",
    "    * Informatica\n",
    "    * SAP\n",
    "    * SAS\n",
    "    \n",
    "    \n",
    "* Data Visualization - These comprise of programming libraries that can be used to create visualizations and tools that inherently provide visualization capabilities. Some of the common tools are \n",
    "     * Tableau\n",
    "     * PowerBI\n",
    "\n",
    "\n",
    "* Model Building -\n",
    "    * Google Cloud\n",
    "\n",
    "     \n",
    "* Model Deployment - Once the model is developed, the model should be consumable by others(via APIs)\n",
    "    * SPSS modeler\n",
    "    * SAS enterprise miner\n",
    "    \n",
    "    \n",
    "    \n",
    "* Model monitoring - Once a model is deployed, imp to track performance as new data arrives in order to check if the model is outdated or not.\n",
    "    * AWS SageMaker\n",
    "    \n",
    "    \n",
    "* Code asset management - \n",
    "    * git based platforms\n",
    "    \n",
    "    \n",
    "* Data asset management - \n",
    "    * Informatica\n",
    "    \n",
    "    \n",
    "* Development Environment - \n",
    "    * IBM Watson studio\n",
    "    \n",
    "    \n",
    "* Execution Environments - \n",
    "    * IBM Watson\n",
    "    \n",
    "    \n",
    "* Fully integrated visual tools - These are tools which help in many or all of the data science tasks define above and also do not require must of programming knowledge to use. Some of them are - \n",
    "    * Watson studio + Watson open scale\n",
    "    * h2o.ai\n",
    "    * Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Languages\n",
    "\n",
    "In data Science, it is not necessary to learn code but learning so can help a lot. There are mainly three languages that are used most commonly in data science domain, these are - \n",
    "\n",
    "* Python\n",
    "* R\n",
    "* SQL\n",
    "* JavaScript\n",
    "\n",
    "Other languages are - scala, java, c++, julia, go, js, ruby, php, visual basic, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries(Python) - \n",
    "\n",
    "* Scientific computing - \n",
    "    * Pandas - Data structures and tools\n",
    "    * NumPy - Arrays and matrices\n",
    "    \n",
    "    \n",
    "* Visualization - \n",
    "    * Matplotlib - most popular\n",
    "    * Seaborn - high level, based on Matplotlib\n",
    "    \n",
    "    \n",
    "* Machine learning and deep learning - \n",
    "    * Scikit-learn\n",
    "    * Keras - deep learning NN\n",
    "    * TensorFlow - deep learning production and deployment\n",
    "    * PyTorch\n",
    "    \n",
    "    \n",
    "* PySpark - provides functionalities for all above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets - \n",
    "\n",
    "* Definition - Structured collection of data. The data can be in data structures like - \n",
    "    * Tabular data - commonly csv format\n",
    "    * Hierarchial/network data - used to define relation in data\n",
    "    * Raw files - images/audio\n",
    "\n",
    "\n",
    "* Data ownnership - \n",
    "    * Private data - datasets are private because they contain confidential data, private or personal information and are commercially sensitive. They are not generally shared publically.\n",
    "    * Open data - these are datasets that institutions, governments, companies or organizations have made available to the general public for use.\n",
    "\n",
    "\n",
    "* Sources of public datasets - publically available open data sets can be found from multiple data sources such as - \n",
    "    * Open data portal list from around the world - http://datacatalogs.org/\n",
    "    * Government and organizational websites like [UN](https://data.un.org/), [US govt data](https://data.gov) and [Indian govt data](https://data.gov.in/)\n",
    "    * Other sources - \n",
    "        * [Kaggle](https://kaggle.com/datasets)\n",
    "        * [Google](https://datasetsearch.research.google.com/)\n",
    "        \n",
    "        \n",
    "* License consideration - While using a publically available dataset(or for that matter any dataset), it is important to check for the license and whether it allows data usage as we require. A common data sharing license is CDLA(Common Data License Agreement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning models -\n",
    "\n",
    "Data contains wealth of information. Manual analysis of data is not possible today since the amount of data present is huge and traditional data analysis techniques are unable to handle the volume. Machine Learning models help identify the patterns in data. Model training is the process through which a machine learning models learns about the data. Once the model is trained, it can be used to make predications on new data. \n",
    "\n",
    "There are 3 basic classes of machine learning- \n",
    "\n",
    "* Supervised learning - most common type, input data and correct output is provided. The model tries to find relation between input data and output data. Generally used for regression and classification.\n",
    "* Unsupervised learning - data is not labeled(not known what is input, structure, type, etc). The model tries to identify pattern in this kind of data without any help from the humans. Generally used for clustering problems and anamoly detection.\n",
    "* Reinforcement learning - similar to way humans learn, by trial and error in idnetifying actions in order to maximize reward over time. \n",
    "\n",
    "Deep learning is specialized type of machine learning. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
